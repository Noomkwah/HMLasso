{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of the Lasso With High Missing Rate."
      ],
      "metadata": {
        "id": "wm5MwpWa5oim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this notebook is to implement the lasso with high missing rate described [here](https://www.ijcai.org/proceedings/2019/0491.pdf). "
      ],
      "metadata": {
        "id": "BMpYFpPn5v0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "ZZL0jO1Q7HvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import cvxpy as cp\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "pVnwGrET6JQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HMLasso"
      ],
      "metadata": {
        "id": "6FEfUyv87Lsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ERRORS_HANDLING = \"raise\"\n",
        "\n",
        "class HMLasso():\n",
        "  \"\"\"\n",
        "  Lasso regularization that performs well with high missing rate.\n",
        "\n",
        "  Implemented according to the related article 'HMLasso: Lasso with High Missing\n",
        "  Rate' by Masaaki Takada1, Hironori Fujisawa and Takeichiro Nishikawa.\n",
        "  Link to the article: https://www.ijcai.org/proceedings/2019/0491.pdf\n",
        "\n",
        "  ------------\n",
        "  Common uses: Once fitted, the HMLasso can provide linear predictions. \n",
        "  It can also be used to select variables of interest from the given data. This \n",
        "  second goal can be achieved through selection of variables whose coefficient\n",
        "  is almost (or equal to) zero. For fitting purpose, X and y must have mean = 0. \n",
        "\n",
        "  Please note that no metric is implemented in this class for now. \n",
        "  See sklearn.metrics.mean_squared_error or like for useful metrics.\n",
        "\n",
        "  ------------\n",
        "  Common error: During the fitting HMLasso.fit(X, y), errors such as\n",
        "  'ArpackNoConvergence: ARPACK error -1: No convergence' may occur. It comes \n",
        "  from the fact that the underlying solver used did not successfully assess\n",
        "  the positive-semidefiniteness of the inner variable Sigma_opt. If you are \n",
        "  sure that Sigma_opt is PSD (which is likely to be the case in normal uses of \n",
        "  the estimator), you can add the two following lines:\n",
        "  \"\n",
        "  from file_04_HMLasso import ERRORS_HANDLING\n",
        "  ERRORS_HANDLING = 'ignore'\n",
        "  \"\n",
        "  If the problem persists, then maybe praying god is the only remaining thing\n",
        "  you can do.\n",
        "\n",
        "  ------------\n",
        "  Parameters:\n",
        "      mu : float/int, default=1.0: the hyperparameter that control how\n",
        "      parcimonious the model shall be. The larger mu is, the greater the\n",
        "      regularization will be (hence the calculated beta_opt might \n",
        "      present more nullified coefficients). mu must be positive.\n",
        "      \n",
        "      alpha : float/int, default=1: the hyperparameter that control weights\n",
        "      importance. Be wary that setting alpha > 5 can make convergence way\n",
        "      slower, as the weights become closer and closer to 0 and as the numerical\n",
        "      solver has more and more trouble converging.\n",
        "      One may prefer setting alpha in the range [0., 3.]. Common values\n",
        "      of alpha are 0., 0.5, 1. with the latter experimentally delivering best\n",
        "      performances. alpha must be positive.\n",
        "      See source article for more.\n",
        "\n",
        "      fit_intercept : bool, default=False: control whether the constant predictor\n",
        "      is used to fit the model. NOT IMPLEMENTED YET.\n",
        "\n",
        "      verbose : bool, default=False: control whether the verbose is dispayed.\n",
        "      Set verbose = True for it to be printed.\n",
        "  \n",
        "  ------------\n",
        "  Methods:\n",
        "      fit(self, X, y, errors=\"ignore\"):\n",
        "        Fit the HMLasso on (X, y)\n",
        "        X, the features, must be a mean-centered numpy array of shape (n, p)\n",
        "        y, the labels, must be a mean-centered vector of shape (n, 1) or (n,)\n",
        "\n",
        "        Do not return anything. However, once the fitting is done, one can\n",
        "        use 'predict' method to predict any given output using the linear model.\n",
        "      \n",
        "      predict(self, X):\n",
        "        Predict using linear model.\n",
        "        Return the predicted vector.\n",
        "  \n",
        "  ------------\n",
        "  Constants:\n",
        "      beta_opt: the estimator.\n",
        "  \"\"\"\n",
        "\n",
        "  global ERRORS_HANDLING\n",
        "\n",
        "  def __init__(self, mu=1, alpha=1, fit_intercept=False, verbose=False):\n",
        "\n",
        "    assert isinstance(mu, (int, float)), \"mu must be a number.\"\n",
        "    assert isinstance(alpha, (int, float)), \"alpha must be a number.\"\n",
        "    assert isinstance(fit_intercept, bool), \"fit_intercept must be a boolean.\"\n",
        "    assert isinstance(verbose, bool), \"verbose must be a boolean.\"\n",
        "    assert mu >= 0, \"mu must be a positive number.\"\n",
        "    assert alpha >= 0, \"alpha must be a positive number.\"\n",
        "\n",
        "    self.mu = mu\n",
        "    self.alpha = alpha\n",
        "    self.verbose = verbose\n",
        "    self.fit_intercept = fit_intercept # Unused at the moment.\n",
        "    \n",
        "    self.n = None\n",
        "    self.p = None\n",
        "    self.S_pair = None\n",
        "    self.rho_pair = None\n",
        "    self.R = None\n",
        "    self.Sigma_opt = None\n",
        "    self.beta_opt = None\n",
        "    self.coef_ = None # Unused at the moment.\n",
        "    self.intercept_ = None # Unused at the moment.\n",
        "\n",
        "    self.isFirstProblemSolved = False\n",
        "    self.isSecondProblemSolved = False # Unused at the moment.\n",
        "    self.isFitted = False\n",
        "  \n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    Predict using the linear model.\n",
        "\n",
        "    ------------\n",
        "    Parameters:\n",
        "        X : 2D numpy array\n",
        "\n",
        "    Returns:\n",
        "        y : 1D numpy array\n",
        "    \"\"\"\n",
        "\n",
        "    assert self.isFitted, \"The model has not yet been fitted.\"\n",
        "    assert X.shape[1] == self.p, f\"Given data is of dimension {X.shape[1]}. Must have dimension {self.p-1}).\"\n",
        "    assert not np.isnan(X).any(), \"Input contains NaN.\"\n",
        "\n",
        "    return np.dot(X, self.beta_opt)\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    \"\"\"\n",
        "    Fit the HMLasso on (X, y).\n",
        "\n",
        "    ------------\n",
        "    Parameters:\n",
        "        X : 2D numpy array, shape (n,p). It corresponds to the features, and\n",
        "        must be mean-centered.\n",
        "        y : 1D numpy array, shape (n,1) or (n,). It corresponds to the labels,\n",
        "        and must be mean-centered.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    \n",
        "    assert type(X) == np.ndarray, \"Features are not a numpy array.\"\n",
        "    assert type(y) == np.ndarray, \"Labels are not a numpy array\"\n",
        "    assert X.shape[0] == y.shape[0], \"Features and labels shapes are not compatibles.\"\n",
        "    assert len(y.shape) == 1, \"Labels are not a vector.\"\n",
        "\n",
        "    self.n, self.p = X.shape    \n",
        "    self.__verify_centering__(X, y)\n",
        "    self.S_pair, self.rho_pair, self.R = self.__impute_params__(X, y)\n",
        "    self.Sigma_opt = self.__solve_first_problem__()\n",
        "\n",
        "    # It appears that, due to floating points exceptions, Sigma_opt is not always\n",
        "    # Positive semidefinite. Hence, we shall check it.\n",
        "    eigenvalues = np.linalg.eig(self.Sigma_opt)[0]\n",
        "    min_eigenvalue = min(eigenvalues)\n",
        "    if min_eigenvalue < 0:\n",
        "      print(f\"[Warning] Sigma_opt is not PSD, its minimum eigenvalue is {min_eigenvalue}. Error handled by adding {-min_eigenvalue} to each eigenvalue.\")\n",
        "      self.Sigma_opt = self.Sigma_opt - min_eigenvalue * np.eye(self.p, self.p)\n",
        "    \n",
        "    # Sigma_opt may ill-typed data: some coefficients may appear comlex-valued\n",
        "    # while they are not. We fix this issue. Example: 5.23 + 0.0j becomes 5.23.\n",
        "    self.Sigma_opt = np.real(self.Sigma_opt)\n",
        "\n",
        "    if ERRORS_HANDLING == \"ignore\":\n",
        "      self.Sigma_opt = cp.psd_wrap(self.Sigma_opt)\n",
        "    \n",
        "    self.beta_opt = self.__solve_second_problem__()\n",
        "\n",
        "    self.isFitted = True\n",
        "\n",
        "    if self.verbose:\n",
        "      print(\"Model fitted.\")\n",
        "\n",
        "  def __verify_centering__(self, X, y, tolerance=1e-8):\n",
        "    for col in range(self.p):\n",
        "      current_mean = X[:, col].mean()\n",
        "      if abs(current_mean) > tolerance:\n",
        "        raise Exception(f\"Data is not centered: column {col} has mean of {current_mean}\")\n",
        "    if abs(y.mean()) > tolerance:\n",
        "        raise Exception(f\"Target is not centered: mean = {y.mean()}\")\n",
        "\n",
        "  def __impute_params__(self, X, y):\n",
        "    if self.verbose:\n",
        "      print(\"[Imputing parameters] Starting...\")\n",
        "\n",
        "    Z = np.nan_to_num(X)\n",
        "    Y = (Z != 0).astype(int)\n",
        "    R = np.dot(Y.T, Y)\n",
        "    if self.verbose:\n",
        "      print(\"[Imputing parameters] R calculated.\")\n",
        "\n",
        "    rho_pair = np.divide(np.dot(Z.T, y), R.diagonal(), out=np.zeros((self.p,)), where=(R.diagonal()!=0))\n",
        "    if self.verbose:\n",
        "      print(\"[Imputing parameters] rho_pair calculated.\")\n",
        "\n",
        "    S_pair = np.divide(np.dot(Z.T, Z), R, out=np.zeros((self.p, self.p)), where=(R!=0))\n",
        "    if self.verbose:\n",
        "      print(\"[Imputing parameters] S_pair calculated.\")\n",
        "\n",
        "    R = R / self.n\n",
        "\n",
        "    if self.alpha > 5:\n",
        "      print(\"[Warning] The hyperparameter alpha={} is large (greater than 5), which might make convergence way slower.\")\n",
        "    R = np.power(R, self.alpha)\n",
        "\n",
        "    if self.verbose:\n",
        "      print(\"[Imputing parameters] Parameters imputed.\")\n",
        "\n",
        "    return S_pair, rho_pair, R\n",
        "\n",
        "\n",
        "  def __solve_first_problem__(self):\n",
        "    \n",
        "    assert self.S_pair is not None, \"Pairwise covariance matrix of features is not determined.\"\n",
        "    assert self.rho_pair is not None, \"Pairwise covariance vector of features and labels is not determined.\"\n",
        "    assert self.R is not None, \"Weights are not determined.\"\n",
        "\n",
        "    if self.verbose:\n",
        "      print(\"[First Problem] Starting...\")\n",
        "\n",
        "    Sigma = cp.Variable((self.p, self.p), PSD = True) # Variable to optimize\n",
        "    obj = cp.Minimize(cp.sum_squares(cp.multiply(self.R, Sigma-self.S_pair))) # Objective to minimize\n",
        "    constraints = [Sigma >> 0] # Constraints: We want Sigma to be positive semi-definite.\n",
        "    if self.verbose:\n",
        "      print(\"[First Problem] Objective and constraints well-defined.\")\n",
        "\n",
        "    # Solve the optimization problem\n",
        "    prob = cp.Problem(obj, constraints)\n",
        "    prob.solve(solver=cp.SCS, verbose=self.verbose)\n",
        "    if self.verbose:\n",
        "      print(f\"[First Problem] Problem status: {prob.status}.\")\n",
        "    if self.verbose:\n",
        "      print(\"[First Problem] Problem solved.\")\n",
        "\n",
        "    self.isFirstProblemSolved = True\n",
        "\n",
        "    return Sigma.value\n",
        "\n",
        "  def __solve_second_problem__(self):\n",
        "    \n",
        "    assert self.S_pair is not None, \"Pairwise covariance matrix of features is not determined.\"\n",
        "    assert self.rho_pair is not None, \"Pairwise covariance vector of features and labels is not determined.\"\n",
        "    assert self.R is not None, \"Weights are not determined.\"\n",
        "    assert self.isFirstProblemSolved, \" First optimization problem has not been solved.\"\n",
        "    assert self.Sigma_opt is not None, \"Sigma_opt is unknown. First optimization problem might have not been solved.\"\n",
        "\n",
        "    if self.verbose:\n",
        "      print(\"[Second Problem] Starting...\")\n",
        "\n",
        "    beta = cp.Variable(self.p) # Variable to optimize\n",
        "    obj = cp.Minimize(0.5 * cp.quad_form(beta, self.Sigma_opt) - self.rho_pair.T @ beta + self.mu * cp.norm1(beta)) # Objective to minimize\n",
        "    constraints = [] # Constraints\n",
        "    if self.verbose:\n",
        "      print(\"[Second Problem] Objective and constraints well-defined.\")\n",
        "\n",
        "    # Solve the optimization problem\n",
        "    prob = cp.Problem(obj, constraints)\n",
        "    prob.solve(solver=cp.SCS, verbose=self.verbose)\n",
        "    if self.verbose:\n",
        "      print(f\"[Second Problem] Problem status: {prob.status}.\")\n",
        "    if self.verbose:\n",
        "      print(\"[Second Problem] Problem solved.\\n\")\n",
        "    \n",
        "    self.isSecondProblemSolved = True\n",
        "\n",
        "    return beta.value"
      ],
      "metadata": {
        "id": "sSdRL-Gm9xgh"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "b6zl4SdSi4TZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_Xy(n, p, replace_rate=0.3):\n",
        "  X = 100*(np.random.rand(n,p)-0.5) # Generate random X\n",
        "  \n",
        "  if p > 3:\n",
        "    y = 7*X[:, 0] - 2 * X[:, 1] + 5 * X[:, 2] + 19 * X[:, 3] + 6*X[:, 4]\n",
        "  else:\n",
        "    y = 7*X[:, 0] - 2 * X[:, 1]\n",
        "  \n",
        "  indices = np.full(X.shape, False, bool)\n",
        "  mask = np.random.choice([False, True], size=X.shape, p=((1 - replace_rate), replace_rate))\n",
        "  X[mask] = np.nan\n",
        "\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "5mmg3-ifVCFy"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  X, y = get_Xy(10000, 20, 0.4)\n",
        "\n",
        "  scaler = StandardScaler(with_std=False)\n",
        "  X_scaled = scaler.fit_transform(X)\n",
        "  lasso = HMLasso(mu=1, alpha=1, verbose=False)\n",
        "  lasso.fit(X_scaled, y-y.mean())\n",
        "  X_test, y_test = get_Xy(10000, 20, replace_rate=0.)\n",
        "  residuals = y_test - lasso.predict(X_test)\n",
        "  print(abs(residuals).mean(), np.sqrt(sum(residuals**2/len(residuals))))\n",
        "  print(lasso.beta_opt.round(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzjr9L-GqN4O",
        "outputId": "979e7998-b6e9-4764-bd55-b974f6f63ce2"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28.38686312043213 35.49830392039553\n",
            "[ 7.3 -2.2  5.4 19.3  6.3 -0.3  0.5 -0.   0.2  0.1 -0.2  0.2 -0.2  0.6\n",
            "  0.1  0.3  0.2 -0.2  0.1 -0.2]\n"
          ]
        }
      ]
    }
  ]
}